\documentclass[12pt]{article}
\usepackage[margin=0.7in]{geometry}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{float}
\usepackage{subfig}
\usepackage[]{algorithm2e}
\RestyleAlgo{boxruled}
\usepackage{rotating}
\usepackage{natbib}

\newcommand{\deriv}[3][]{% \deriv[<order>]{<func>}{<var>}
  \ensuremath{\frac{\partial^{#1} {#2}}{\partial {#3}^{#1}}}}
\newcommand{\atconstant}[2]{\ensuremath{\left( #1 \right)_{#2}}}
\begin{document}
\title{PHYS 849: Assignment 5}
\author{Jake Bauer}
\date{\today}
\maketitle

\section*{Problem 1}

\subsection*{(a)}
We are asked to go through the procedure of testing a hypothesis about the mean of this distribution from the data.  To explore this question, we generate 10 random data points from a Gaussian with $\mu = 100$ and $\sigma = 10$.  The log-likelihood function for a general Gaussian PDF of unknown mean $\mu$ and known standard deviation
$\sigma$ is given by,
\begin{equation} \label{eq:gauss_hypothesis}
\ln \mathcal{L} = -\frac{n}{2 \sigma^2} \mu \left[\mu - 2 \overline{x}\right]
\end{equation} 
where $\overline{x}$ is the data sample mean, and where we have dropped factors that do not depend on the unknown quantities.  Such terms would just correspond to constant shifts of the likelihood, and are consequently negligible.  We are then asked to test:
\begin{itemize}
\item $H_0$: $\mu = 100$
\item $H_1$: $\mu \neq 100$
\end{itemize}
where $H_0$ and $H_1$ are the null and alternate hypotheses.  To compare the relative likelihood of these hypotheses, we construct a test statistic,
\begin{equation}
\ln \Lambda = \ln \frac{\mathcal{L}_0}{\mathcal{L}_1},
\end{equation}
or the difference in log-likelihoods. For the null hypothesis log-likelihood, we take Eq. \eqref{eq:gauss_hypothesis} with $\mu = 100$.  For $H_1$, we know that $\overline{x}$ is an unbiased and consistent estimator of $\mu$, so we take $\mu = \overline{x}$.  If $\mu = \overline{x}$, the test statistic is minimized and the hypotheses are equivalent.  Alternatively, a high $\ln \Lambda$ means the null hypothesis is preferred and a low one means the opposite.  To test these hypotheses, we draw $10^4$ samples of 10 from the null hypothesis distribution in addition to the original data set. These form the basis of a Monte Carlo analysis in which the test statistic is computed for a range of $\overline{x}$.  This distribution plotted in Figure \ref{fig:hypothesis_1}, and the value generated by the original sample is shown by the position of the dashed line.  The p-value of this observation is 0.3618, so we fail to reject the null hypothesis.

\subsection*{(b)}
\begin{figure}
\includegraphics[width=0.9\textwidth]{hypothesis_1} \caption{The test statistic for $H_0 = 100$, $H_1 \neq 100$.  The prediction from Wilk's Theorem is overlaid. The sample data set's value of $-2 \ln \Lambda$ is denoted by the dashed line.} \label{fig:hypothesis_1}
\end{figure}
We will follow the same procedure to test,
\begin{itemize}
\item $H_0$: $\mu = 150$
\item $H_1$: $\mu \neq 150$
\end{itemize}
We repeat the analysis from the previous section, and the position of the same sample of 10 from the original $\mu=100$ distribution is shown by the dashed line.  The p-value of this observation is 0.0634, suggesting this result is still not significant at a $5\%$ level.
\begin{figure}
\includegraphics[width=0.9\textwidth]{hypothesis_2}\caption{The test statistic for $H_0 = 150$, $H_1 \neq 150$.  The prediction from Wilk's Theorem is overlaid. The sample data set's value of $-2 \ln \Lambda$ is denoted by the dashed line.} \label{fig:hypothesis_2}
\end{figure}

\subsection*{(c)}
Wilk's theorem states that in the large sample size limit that $-2 \ln \Lambda$ asymptotically tends to a $\chi^2$ distribution with degrees of freedom equal to the difference in dimensionality of the hypotheses.   In both cases we have studied, the null hypothesis is a point of dimension zero, and the alternative hypothesis is the real number line with a hole.  Thus, we expect the $-2 \ln \Lambda$ to follow a $\chi^2$ distribution with 1 degree of freedom.  In Figures \ref{fig:hypothesis_1} and \ref{fig:hypothesis_2}, this is shown by a red line.  The p-values predicted for the hypotheses are 0.3822 and 0.0668, respectively computed from the $\chi^2$ CDF.  This shows very good agreement despite only having 10 samples.
\section*{Problem 2}
\subsection*{(a)}
We are asked to find the properties of the 90\% confidence interval on the number of counts expected in a Poisson-distributed measurement.  The number observed, $n = \hat{\mu}$, is the maximum likelihood estimator for $\mu$, the expected number of counts in each bin.  The confidence interval $[a,b]$ is in general determined implicitly from the equations \citep{cowan1998statistical},
\begin{eqnarray}
\alpha &=& \int_{\hat{\mu}}^{\infty} f(\hat{\mu} \vert a) \text{d} \hat{\mu}\\
\beta &=& \int_{- \infty}^{\hat{\mu}} f(\hat{\mu} \vert b) \text{d} \hat{\mu}
\end{eqnarray}
where $\alpha$ and $\beta$ are the right and left significance probabilities, and $f(\hat{\mu})$ is the PDF of the estimator.  Following the argument in \citet{cowan1998statistical}, we evaluate these integrals in semi-analytic fashion in terms of known functions. Starting with the first. and since the Poisson distribution is discrete, we begin with,
\begin{eqnarray}
\alpha &=& 1 - \sum_{k = 0}^{\hat{\mu} - 1} \frac{a^{k}}{k!} e^{-a}\\
&\geq& 1 - \int_{2 a}^{\infty} g_{\chi^2}(\psi \vert \text{df}=2\hat{\mu}) \text{d} \psi \\
&=& 1 - G_{\chi^2}(2 a \vert \text{df}=2\hat{\mu} )
\end{eqnarray}
where $g_{\chi^2}$ and $G_{\chi^2}$ are the $\chi^2$ PDF and CDF respectively.  The second step follows from the fact that each term in the Poisson distribution has the same form as the $\chi^2$ PDF, and as the mean becomes large, the sum approaches the integral.  Another interesting point that stems from this is the fact that the interval will be overcovered.  This simply arises from how the $\chi^2$ CDF bounds the Poisson sum.  We can invert and solve for $a$ trivially as,
\begin{equation} \label{eq:a}
a = \frac{1}{2} G^{-1}_{\chi^2} (\alpha \vert \text{df}=2\hat{\mu}).
\end{equation}
We can repeat this logic for the $\beta$ sum,
\begin{eqnarray}
1 - \beta &=& 1 - \sum_{k = 0}^{\hat{\mu}} \frac{b^{k}}{k!} e^{-b}\\
&\leq& 1 - \int_{2 b}^{\infty} g_{\chi^2}(\psi \vert \text{df}=2(\hat{\mu} + 1)) \text{d} \psi \\
&=& 1 - G_{\chi^2}(2 b \vert \text{df}=2(\hat{\mu} + 1) ),
\end{eqnarray}
giving,
\begin{equation} \label{eq:b}
b = \frac{1}{2} G^{-1}_{\chi^2} (1 - \beta \vert \text{df}=2(\hat{\mu} + 1)).
\end{equation}
We now wish to consider three intervals of confidence level, $1 - \gamma = 0.9$. The first interval is symmetric with $\alpha = \beta = 0.5 \gamma$.  The other two, the upper and lower limit intervals, take either $\alpha = 0.1$ or $\beta = 0.1$ respectively.  From Eqs. \eqref{eq:a} and \eqref{eq:b}, we determine that for the symmetric interval, 
\begin{equation}
[a_{sym},b_{sym}] = \left[\frac{1}{2} G^{-1}_{\chi^2} (0.05 \vert \text{df}=2\hat{\mu}), \frac{1}{2} G^{-1}_{\chi^2} (0.95 \vert \text{df}=2(\hat{\mu} + 1))\right],
\end{equation}
for the upper limit interval,
\begin{equation}
b_{up} =  G^{-1}_{\chi^2} (0.9 \vert \text{df}=2(\hat{\mu} + 1)),
\end{equation}
and for the lower limit interval, 
\begin{equation}
a_{low} = \frac{1}{2} G^{-1}_{\chi^2} (0.1 \vert \text{df}=2\hat{\mu}).
\end{equation}

\begin{figure}
\includegraphics[width=0.9\textwidth]{confidence_belt_distribution.eps}
\caption{The distribution of $b$ (blue) and $a$ (green) computed from Eqs. \eqref{eq:a} and \eqref{eq:b} for a symmetric confidence interval and $10^5$ draws.  } \label{fig:symmetric}
\end{figure}

\begin{figure}
\includegraphics[width=0.9\textwidth]{confidence_belt_distribution_upper_lower.eps}
\caption{The distribution of $b$ (blue) and $a$ (green) computed from Eqs. \eqref{eq:a} and \eqref{eq:b} for the upper limit and lower limit intervals and $10^5$ draws.  } \label{fig:one_sided} 
\end{figure}

\subsection*{(b)}
To test the validity of Eqs. \eqref{eq:a} and \eqref{eq:b}, we draw $10^5$ samples from a Poisson distribution of mean 5.7.  For each of these we compute the upper and lower limits for the symmetric and limiting intervals at $\gamma = 0.1$.  Figure \ref{fig:symmetric} shows the distribution of $a$ and $b$ for the symmetric interval.  The results are broadly in agreement with the tabulated values in \citet{cowan1998statistical}.  Similarly, for the upper and lower one-sided intervals, we make the same plot in Figure \ref{fig:one_sided}.  Again, these intervals broadly agree with tabulated values for a Poisson distribution. 

An interesting point to make is that the confidence intervals actually cover $\gamma_{sym} = 0.94736$, $\gamma_{low} = 0.93520$, and $\gamma_{up} = 0.92319$.  The fact that we have overcovered is a known effect that occurs due to the discrete nature of the Poisson distribution.  When we use the $\chi^2$ CDF to compute these quantities, we are using an upper bound on the value of the sum.  The discrepancy disappears if one takes higher $\mu$, since the relative spacing between significant points in the CDF becomes small. That is to say, the CDF itself becomes broader, and a $\Delta \mu$ of 1 is no longer as large.

Another interesting remark is the case where $n = 0$.  This can happen quite easily if the mean is small, but only a minor modification to the calculation is needed.  In this case, we are only able to establish an upper bound, which we do at a level $\beta$.  The calculation for $b$ is the same, and we take $a = 0$.  

\bibliographystyle{plainnat}
\bibliography{bibliography}
\end{document}